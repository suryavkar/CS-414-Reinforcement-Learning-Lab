{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c33283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d495f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "EPISODES = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "EXPLORATION_MIN = 0.001\n",
    "\n",
    "FC1_DIMS = 1024\n",
    "FC2_DIMS = 512\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8b27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "rewards = []\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_shape, FC1_DIMS)\n",
    "        self.fc2 = nn.Linear(FC1_DIMS, FC2_DIMS)\n",
    "        self.fc3 = nn.Linear(FC2_DIMS, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.mem_count = 0\n",
    "        \n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=bool)\n",
    "    \n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        mem_index = self.mem_count % MEM_SIZE\n",
    "        \n",
    "        self.states[mem_index]  = state\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_\n",
    "        self.dones[mem_index] =  1 - done\n",
    "\n",
    "        self.mem_count += 1\n",
    "    \n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "        \n",
    "        states  = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones   = self.dones[batch_indices]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "class DQN_Solver:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "        q_values = self.network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_count < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states , dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(DEVICE)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(DEVICE)\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        q_values = self.network(states)\n",
    "        next_q_values = self.network(states_)\n",
    "        \n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "        \n",
    "        q_target = rewards + GAMMA * predicted_value_of_future * dones\n",
    "\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc01355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 Average Reward 25.0 Best Reward 60.0 Last Reward 12.0 Epsilon 0.9107167220751963\n",
      "Episode 20 Average Reward 23.05 Best Reward 60.0 Last Reward 17.0 Epsilon 0.8195091081446627\n",
      "Episode 30 Average Reward 21.5 Best Reward 60.0 Last Reward 46.0 Epsilon 0.7474612800978938\n",
      "Episode 40 Average Reward 22.275 Best Reward 60.0 Last Reward 45.0 Epsilon 0.6609325183970435\n",
      "Episode 50 Average Reward 22.1 Best Reward 60.0 Last Reward 10.0 Epsilon 0.5938489449248604\n",
      "Episode 60 Average Reward 22.883333333333333 Best Reward 60.0 Last Reward 40.0 Epsilon 0.5193569825542639\n",
      "Episode 70 Average Reward 26.085714285714285 Best Reward 104.0 Last Reward 35.0 Epsilon 0.4140699125642426\n",
      "Episode 80 Average Reward 33.575 Best Reward 148.0 Last Reward 148.0 Epsilon 0.2693272800799172\n",
      "Episode 90 Average Reward 41.91111111111111 Best Reward 200.0 Last Reward 150.0 Epsilon 0.15645860956583554\n",
      "Episode 100 Average Reward 49.89 Best Reward 200.0 Last Reward 137.0 Epsilon 0.08512657986534605\n",
      "Episode 110 Average Reward 61.50909090909091 Best Reward 200.0 Last Reward 196.0 Epsilon 0.03500241729185455\n",
      "Episode 120 Average Reward 71.45833333333333 Best Reward 200.0 Last Reward 200.0 Epsilon 0.01416382139214029\n",
      "Episode 130 Average Reward 80.13846153846154 Best Reward 200.0 Last Reward 200.0 Epsilon 0.005634793339436438\n",
      "Episode 140 Average Reward 88.11428571428571 Best Reward 200.0 Last Reward 200.0 Epsilon 0.0021591629909691198\n",
      "Episode 150 Average Reward 94.34 Best Reward 200.0 Last Reward 118.0 Epsilon 0.001\n",
      "Episode 160 Average Reward 99.8625 Best Reward 200.0 Last Reward 200.0 Epsilon 0.001\n",
      "Episode 170 Average Reward 105.07058823529412 Best Reward 200.0 Last Reward 200.0 Epsilon 0.001\n",
      "Episode 180 Average Reward 109.03333333333333 Best Reward 200.0 Last Reward 163.0 Epsilon 0.001\n",
      "Episode 190 Average Reward 113.40526315789474 Best Reward 200.0 Last Reward 193.0 Epsilon 0.001\n",
      "Episode 200 Average Reward 116.855 Best Reward 200.0 Last Reward 200.0 Epsilon 0.001\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Solver()\n",
    "\n",
    "for i in range(1, EPISODES+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            if i%10==0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            episode_number.append(i)\n",
    "            rewards.append(score)\n",
    "            average_reward_number.append(average_reward/i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf6c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"dqn_cp.csv\",rewards,delimiter =\", \",fmt ='% d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b83ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d023b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
