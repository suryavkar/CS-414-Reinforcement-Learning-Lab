{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6df429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6d157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "# You can move either left or right to balance the pole\n",
    "# Lets implement the Actor critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # 4 because there are 4 parameters as the observation space\n",
    "        self.actor = nn.Linear(128, 2) # 2 for the number of actions\n",
    "        self.critic = nn.Linear(128, 1) # Critic is always 1\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_prob, state_values\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    # We calculate the losses and perform backprop in this function\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses =[]\n",
    "    returns = []\n",
    "    \n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + 0.99 * R # 0.99 is our gamma number\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65543e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f03962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = 10\n",
    "    rew = []\n",
    "    for i_episode in count(): # We need around this much episodes\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0: # We will print some things out\n",
    "            print(\"Episode {}\\tLast Reward: {:.2f}\\tAverage reward: {:.2f}\".format(i_episode, ep_reward, running_reward))\n",
    "        rew.append(ep_reward)\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved, running reward is now {} and the last episode runs to {} time steps\".format(\n",
    "                    running_reward, t))\n",
    "            break\n",
    "    return(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4116e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast Reward: 19.00\tAverage reward: 10.45\n",
      "Episode 10\tLast Reward: 9.00\tAverage reward: 12.77\n",
      "Episode 20\tLast Reward: 104.00\tAverage reward: 22.70\n",
      "Episode 30\tLast Reward: 80.00\tAverage reward: 33.52\n",
      "Episode 40\tLast Reward: 13.00\tAverage reward: 57.37\n",
      "Episode 50\tLast Reward: 16.00\tAverage reward: 40.59\n",
      "Episode 60\tLast Reward: 13.00\tAverage reward: 29.71\n",
      "Episode 70\tLast Reward: 11.00\tAverage reward: 23.98\n",
      "Episode 80\tLast Reward: 27.00\tAverage reward: 22.19\n",
      "Episode 90\tLast Reward: 27.00\tAverage reward: 22.55\n",
      "Episode 100\tLast Reward: 38.00\tAverage reward: 25.60\n",
      "Episode 110\tLast Reward: 47.00\tAverage reward: 35.05\n",
      "Episode 120\tLast Reward: 200.00\tAverage reward: 87.74\n",
      "Episode 130\tLast Reward: 200.00\tAverage reward: 132.79\n",
      "Episode 140\tLast Reward: 101.00\tAverage reward: 136.01\n",
      "Episode 150\tLast Reward: 189.00\tAverage reward: 112.01\n",
      "Episode 160\tLast Reward: 200.00\tAverage reward: 121.19\n",
      "Episode 170\tLast Reward: 200.00\tAverage reward: 152.81\n",
      "Episode 180\tLast Reward: 200.00\tAverage reward: 160.69\n",
      "Episode 190\tLast Reward: 200.00\tAverage reward: 144.40\n",
      "Episode 200\tLast Reward: 200.00\tAverage reward: 158.08\n",
      "Episode 210\tLast Reward: 129.00\tAverage reward: 169.31\n",
      "Episode 220\tLast Reward: 174.00\tAverage reward: 166.89\n",
      "Episode 230\tLast Reward: 200.00\tAverage reward: 179.43\n",
      "Episode 240\tLast Reward: 200.00\tAverage reward: 187.69\n",
      "Episode 250\tLast Reward: 200.00\tAverage reward: 192.63\n",
      "Solved, running reward is now 195.1087671004528 and the last episode runs to 200 time steps\n"
     ]
    }
   ],
   "source": [
    "rew = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4171223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"a2c_cp.csv\",rew,delimiter =\", \",fmt ='% d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682f95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
