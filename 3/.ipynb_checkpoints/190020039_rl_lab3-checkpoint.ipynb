{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773ddfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63eb163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Markov Decision Process model\n",
    "A = {0:\"keep\", 1:\"replace\"};  # actions\n",
    "S = {0:\"low\", 1:\"average\", 2:\"high\"};  # states\n",
    "\n",
    "gamma = 0.85;  # discount factor\n",
    "eps = 1e-6; # error tolerance\n",
    "# Transition probabilities per state-action pair\n",
    "P = np.ones((len(S),len(A),len(S)));\n",
    "P[:,0,:] = np.array([[0.6,0.3,0.1],[0.2,0.6,0.2],[0.1,0.3,0.6]]);\n",
    "P[:,1,:] = P[:,1,:]/3;\n",
    "\n",
    "# Expected Immediate Reward\n",
    "R = np.array([[10.0,9.0],[12.0,11.0],[14.0,13.0]]);\n",
    "# R[:,1] = R[:,1] - 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0841710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma,P,R):\n",
    "    count = 1;\n",
    "    V = np.zeros((P.shape[0],1)); # initial value function\n",
    "    Q = np.zeros(R.shape);\n",
    "    for i in range(P.shape[0]):\n",
    "        for a in range(P.shape[1]):\n",
    "            Q[i,a] = R[i,a];\n",
    "            for j in range(P.shape[2]):\n",
    "                Q[i,a] = Q[i,a] + gamma*P[i,a,j]*V[j,0];\n",
    "    V_new = np.amax(Q,1,keepdims=True);\n",
    "    while np.max(np.abs(V_new-V)) > eps:\n",
    "        count += 1;\n",
    "        V = V_new;\n",
    "        Q = np.zeros(R.shape);\n",
    "        for i in range(P.shape[0]):\n",
    "            for a in range(P.shape[1]):\n",
    "                Q[i,a] = R[i,a];\n",
    "                for j in range(P.shape[2]):\n",
    "                    Q[i,a] = Q[i,a] + gamma*P[i,a,j]*V[j,0];\n",
    "        V_new = np.amax(Q,1,keepdims=True);\n",
    "    print(\"Number of iterations = \",count);\n",
    "#     print(Q);\n",
    "#     print(V_new)\n",
    "    return(np.argmax(Q,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0e1c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations =  102\n",
      "Final policy using value iteration : [1 0 0]\n",
      "Time taken =  0.005595100000000075 s\n"
     ]
    }
   ],
   "source": [
    "t1 = timer();\n",
    "print(\"Final policy using value iteration :\",value_iteration(gamma,P,R));\n",
    "t2 = timer();\n",
    "print(\"Time taken = \",t2-t1,\"s\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491cb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mu,gamma,P,R):\n",
    "    V_mu = np.zeros((P.shape[0],1)); # initial value function\n",
    "    V_mu_new = np.zeros((P.shape[0],1));\n",
    "    for i in range(P.shape[0]):\n",
    "        V_mu_new[i,0] = R[i,mu[i]];\n",
    "        for j in range(P.shape[0]):\n",
    "            V_mu_new[i,0] = V_mu_new[i,0] + gamma*P[i,mu[i],j]*V_mu[j,0];\n",
    "    while np.max(np.abs(V_mu_new-V_mu)) > eps:\n",
    "        for i in range(P.shape[0]):\n",
    "            V_mu[i,0] = V_mu_new[i,0];\n",
    "            V_mu_new[i,0] = R[i,mu[i]];\n",
    "            for j in range(P.shape[0]):\n",
    "                V_mu_new[i,0] = V_mu_new[i,0] + gamma*P[i,mu[i],j]*V_mu[j,0];\n",
    "    return V_mu_new;\n",
    "\n",
    "def policy_improvement(gamma,P,R,V_mu):\n",
    "    Q = R;\n",
    "    for j in range(P.shape[0]):\n",
    "        Q = Q + gamma*P[:,:,j]*V_mu[j,0];\n",
    "#     print(Q)\n",
    "    return(np.argmax(Q,axis=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4d62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma,P,R):\n",
    "    mu = np.ones(P.shape[0],dtype=np.int64); # initial policy\n",
    "    V_mu = policy_evaluation(mu,gamma,P,R);\n",
    "    count = 1;\n",
    "    mu_new = policy_improvement(gamma,P,R,V_mu);\n",
    "    while np.sum(mu != mu_new) > 0:\n",
    "        count += 1;\n",
    "#         mu = np.zeros(len(mu_new));\n",
    "        mu = mu_new;\n",
    "        V_mu = policy_evaluation(mu,gamma,P,R);\n",
    "        mu_new = policy_improvement(gamma,P,R,V_mu);\n",
    "    print(\"Number of iterations = \",count);\n",
    "#     print(V_mu);\n",
    "    return(mu_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc8642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations =  3\n",
      "Final policy from policy iteration : [1 0 0]\n",
      "Time taken =  0.009491300000000091 s\n"
     ]
    }
   ],
   "source": [
    "t3 = timer();\n",
    "print(\"Final policy from policy iteration :\",policy_iteration(gamma,P,R));\n",
    "t4 = timer();\n",
    "print(\"Time taken = \",t4-t3,\"s\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7d599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
